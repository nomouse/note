服务发现和配置管理是大型分布式应用系统的两个重要的基础服务，中间件多年来沉淀下来对应的有影响力的产品是CS，DM和VS，这三款产品随着这些年双11的洗礼，在产品化和可运维性方面都达到了工业产品的能力，也在各自的领域中持续打磨发亮。

本文从软负载的领域的关键模型谈起，看看一款软负载产品有哪些关键的设计关注点，然后以此为锚来看看CS/DM/VS在三款产品在这些技术关注点上是如何对标的。

看完这个文章，你应该拥有了软负载这个领域的common sense了。

说明：CS和VS同属于软负载领域的产品，DM本质上是一款配置管理产品。

按照惯例，正式开始之前，需要有前戏，先做一些背景交代吧。

背景说明
1、从问题开始

先看看以下这些问题，带着问题继续往下看（本文并不尝试回答所有的问题，未竞事宜，自行遍历）

• 服务发现和配置管理的本质区别是什么？
• 硬负载和软负载的区别是什么？
• 负载均衡（Load Balance）和服务发现（Service Discovery）各指什么？
• CS/VS/DM这三个产品在电商全局大图中的什么位置？
• 如果我要设计一个分布式服务发现系统，涉及哪些关键设计领域？
• CS/VS/DM三个产品的一致性协议是？
• 网络分区（Partition）时，三个产品如何应对？
• 三个产品的如何确保高可用？
• 如果你要给服务发现系统确定一个SLA指标，答案是什么？
• CS如何做到不依赖本地文件系统？

说明

• 本文屏蔽了「单元化」等业务相关的概念，纯粹的看标准的软负载，也有利于对问题的澄清和理解。
• 本文只谈「服务发现」和「配置管理」，不谈其他（比如zookeeper的分布式锁，分布式协调服务等）

2、概念交代
• 「服务发现」中的「服务」：可以等同于ip+port；一个服务在运行时，需要活性检查（或者叫健康检查），也就是服务是有状态的（至少包括可用和不可用两种状态）
• 「配置管理」中的「配置」：key=value；配置一经生成，一直存在，不需要活性检查，也就是说配置是没有状态的

很重要的：从CAP的理论来界定，「服务」和「配置」可以不是读强一致的数据；对于一个「服务」和「配置」的分布式系统，其A > C （你可以不认同我的定义，但是本文以此为依据，当然，这个也是我们经过实际实践总结出来的）

3、CS/DM/VS在哪个位置

03.png

描述一下三个系统的主要职责

• VS：主要为统一接入AServer提供域名和ip列表的信息，使得AServer同RealServer（这里一般是一个web 应用）之间可以p2p的直连调用，无需再经过一层lvs集群（所以你如果问我VS存在的目的是什么？我会简单的回答：是为了取代IDC内部的lvs集群）
• CS：主要为RPC调用提供服务的注册和订阅功能，并且对服务进行活性检查
• DM：提供统一的配置管理

说明：在DM未出世以前，CS同时承担了RPC的服务发现和配置管理两个角色（那时候叫CS是合理的），后来DM横空出世，CS就专注在RPC领域了。

三个产品都是pub/sub的模式。

接下来进入正题。

软负载领域模型和设计要点
1、先记住这张图

先记住这张图.png

上图中的一些概念说明

• Service Registry注册中心：负责服务的注册（register）/注销(unregister)/活性检查（KeepAlive）/服务数据查询
• Consumer服务消费者（也可以说服务订阅者）：以某种方式（后面要重点交代不同的方式）从Registry 发现到所需要的服务，而后向Provider发起服务调用
• Service Provider服务提供者
• LB负载均衡（Load Balance）：在Consumer测提供了多种负载算法来确定本次调用选用的ip地址

接下来将按照如下过程来描述软负载领域模型

• Consumer侧的负载均衡有哪些实现模式
• Provider侧的服务注册有哪些模式
• Registry的设计关注点

2、Consumer侧的负载均衡有哪些实现模式

2.1 集中式的LB
LB-1.png

Consumer一般通过一个域名从DNS获取到一个可用的公网IP地址，而后发起访问。

这个方案的特点是：在调用链路上存在一个独立部署的中间设备（一般就是我们所熟知的F5/LVS/HAproxy集群），这些设备其实就是硬负载设备，他们负责对后端的服务进行发现，对流量进行负载均衡。

【方案优势】
• 存在一个统一的流量集中化节点，可以实现一些全局性的掌控：比如实现全站https协议的加解密，对DDoS攻击进行防控等；

【方案劣势】
• 硬负载设备需要实现集群化部署的模式以解决单点故障的问题
• 调用链路性能有一定损耗
• 硬负载设备运维的友好性不足（特别是F5等商业设备更难维护）
• 硬负载设备的成本问题

2.2 进程内LB

LB-2.png

这个方案有如下特点
• 没有了中心化的硬负载设备，把LB的功能以SDK的模式集成到服务消费方进程里（这个模式可以称为"软负载"的模式）
• 引入了注册中心（Servcie Registry），用Registry来动态管理所有的服务地址
• 注册中心不在调用的主链路上，他在旁路

【方案优势】
• Consumer直接调用Provider，不再经过中心节点
• 不需要运维独立的负载均衡设备，也就不存在成本和运维的问题

【方案劣势】
• 对Consumer端有侵入性，存在接入成本
• 以SDK的模式集成，如果要支持不同的程序语言，需要提供不同语言的SDK
• 无中心化，调用管控弱
• 虽然Registry在旁路，但是也是一个关键的基础设施，需要确保高可用

2.3 独立的LB进程

LB-3.png

这个方案同属"软负载"的方案，Consumer端部署独立的LB进程

【方案优势】
• Consumer端无侵入
• Consumer侧独立部署的LB进程，没有多语言实现的成本

【方案劣势】
• 部署成本较高
• 排查问题困难

到这里我要对硬负载和软负载下一个概念的定义
• 硬负载：基于服务端的负载均衡，需要独立部署的负载均衡主机，流量集中（商用F5/LVS/HAProxy）；上述第一个方案就是硬负载的方案
软负载：基于客户端+服务发现+负载均衡；上述第二和第三个方案都属于软负载方案。

硬负载方案和软负载方案在一个大型的分布式应用系统中都还是各自有自己的位置的，比如阿里云的SLB，AWS的ELB都属于硬负载方案的负载集群；软负载方案有淘系的CS，开源界的Eureka。开源界还有很多软负载类的产品（比如zookeeper/etcd/consul等），但这类产品一般不仅提供软负载的服务发现能力，还包括其他的能力。

接下来看看服务提供者这这一端主要设计关注点

3、服务注册有哪种模式

3.1 自注册方式 self registration

自注册.png

这个模式的特点是服务注册模块以SDK的模式嵌入应用的进程中，应用需要主动发起服务注册行为，对于服务注册SDK存在对多语言版本的支持需求。

3.2 第三方注册 third-party registration

第三方注册.png

这个模式的特点是，服务注册模块以独立的进程运行在应用端，可以做到应用无需主动进行服务注册

当然，也可以不依赖于客户端的方式，比如注册中心提供RESTful协议的注册接口，应用根据规范调用注册中心的API完成相关的服务注册动作。

Provider端的行为相对比较简单。CS采用的是第一种自注册的模式，开源界容器调度解决方案k8s中的kube-proxy就属于第三方注册的模式。

接下来进入重头戏，来看看注册中心Registry部分

4、注册中心Servcie Registry设计关注点

注册中心Servcie Registry设计关注点.png

Registry是一个分布式的应用，我们有如下关注点

• 一致性协议
• 对外提供的API
• 服务变更的通知模式
• 容灾策略
• 可用性保证

下面一个个阐述

4.1 Service Registry 的几种分布式一致性协议（自行脑补分布式一致性解决的问题吧）

4.1.1 单机模式+file cache

单机模式+file cache.png

【技术特点】
• 单机部署，用本地磁盘持久化数据；服务数据变化的时候先更新磁盘，再更新内存
• 一致性容易实现：锁/队列方式
【优势】
• 容易实现
【缺陷】
• 单机容量不足
• 容灾能力不足

4.1.2 Multi Server+DB

Multi Server+DB.png

【技术特点】
• 多个Registry组成集群，无状态
• 一致性：直接复用DB的一致性特性
【优势】
• Registry无状态实现简单，容易维护，可靠

【缺陷】
• 引入DB依赖，相对繁重
• DB性能瓶颈：Pub写频繁/sub查询频繁
• DB的可用性决定Registry的可用性

4.1.3 Multi Server+file cache+DB

Multi Server+file cache+DB.png

【技术特点】

• 前一种模式的优化版本：registry+本地缓存
• 利用DB来满足一致性
• 需要有file cache和db的一致性机制

4.1.4 Multi-Server+file cache

Multi-Server+file cache.png

【技术特点】
• 多个server组成集群
• 每一个server用本地的磁盘来持久化
【优势】
• 多副本，不存在单点
• 每一个service Registry都可以独立对外提供服务
【劣势】
• 多副本的分布式一致性算法实现复杂
• 集群自管理（扩容/缩容）

这个方案最大的特点是脱离了DB，但是需要自己实现server的数据一致性协议，业务有很多典型的算法（比如paxos/multi-paxos/raft/zab等）

上述不同的方案的阐述顺序，其实也是系统的发展过程中的一个真实的架构演进顺序。

4.2 Service Registry API

• 服务注册：register publisher
• 服务注销：unregister publisher
• 健康检查：checkHealth
• 服务发现：sub register
• 服务治理相关的一些查询接口

4.3 Service Registry 服务变更的通知模式

4.3.1 push模式
• Regsitry主动推送变更数据给Consumer
• 时效性强

4.3.2 Pull模式
• consumer主动轮询
• 时效性相对较弱，Regsitry的无谓的查询消耗

4.3.3 push&pull结合的模式

至此，基本上阐述了一个软负载产品的领域模型和设计关注点；对于Registry的容灾策略，高可用保证接下来将结合具体的产品来阐述

CS
1、CS总览

CS总览.png

• sdk集成模式，同app在同一进程内
• 目前支持java，cpp，nodejs
• sdk同Regsitry间自定义通信协议，长链接
• sdk上报心跳，双向可用性检测（健康检查）
• 服务变更主动push给sdk
• CS集群不依赖DB，没有file cache

2、CS的一致性协议：Master/Master

CS的一致性协议：Master:Master.png

CS不依赖于DB，自己实现了一致性协议；这个协议的特点是：异步复制+renew+self check

• HSF在应用端向ConfigClient注册pub（1到2步）
• ConfigClient随机注册到一个master registry（3到4步）
1）master宕机，自动重注册到集群中另外可用的Registry
2）master接收注册，即可返回
• master异步复制pub给其他所有的slave（5步）
• master：renew（第6步）
1）master定期向slave发送renew报文
2）slave的pub的alive time被刷新
3）可以发现slave上缺失的pub（如：扩容）
• slave：self check（第7步）
1）slave周期性自检本地所有的cluster provider
2）alive time超期则自动删除

3、CS容灾：网络分区（Partition）

CS容灾：网络分区（Partition.png

• 正常情况下，registry部署在et2和eu13机房，两两互写达到多副本数据的一致性
• 当et2和eu13出现partition时，eu2和eu13断裂成小集群，self check会清除另一个机房的cluster provider
• et2和eu13机房内部的rpc调用不受影响
• Partition期间，CS集群一样可以读写，可用性不受影响
• 网络恢复后，自动愈合

4、CS可用性：客户端多级缓存（cache）

CS可用性：客户端多级缓存（cache）.png

• configclient在memory和file中各缓存一份服务地址
• Registry的服务地址push给configclient时，先写file，再更新memory，运行时以memory为主
• Consumer重启的时候，根据file cache来初始化memory，Registry随后推送最新的数据给configclient
• 如果Registry集群重启/宕机的时候，configclient中memory的服务数据保持不变，持续可用

5、CS架构演进

5.1 2.0 两两互写的模式

CS 架构演进：2.0 两两互写.png

• 两两互写模式，既是master，也是slave
• 单机存储全量pub数据，内存瓶颈
• 集群越大，网卡瓶颈，推送能力下降
• 彼此感知，扩容步骤复杂，风险高
• 应用发布时大量写入，集群互写压力大
• 为了保证推送效率，物理机

5.2 3.0 分层 session+data

CS 架构演进：3.0 分层 session+data.png

分层：职责单一化
• session接受sdk的pub和sub注册
• session多写给data，data存储全量pub数据
• session同data的一致性协议同2.0
• data聚合所有的数据压缩后推送给session
• session缓存已经压缩过的pub数据
• session直接推送数据给sub
• session彼此不感知，data彼此不感知
• Session主动感知data
• 推送能力
1）Sub主动订阅3s内推送到位
2）pub变更3~6s内推送到位

关于这部分的更多详情可以查看
1、https://www.atatech.org/articles/66788
2、https://www.atatech.org/articles/67459

VS
1、VS总览

VS总览.png

• Server端支持RESTful协议
• Consumer端可以通过sdk/agent集成
• sdk目前支持java，cpp，PHP
• VS主动心跳检查provider
• 服务变更push+pull
• 集群不依赖DB，数据存磁盘、每台机器都数据对等

2、VS的一致性协议

2.1 1.0版本基于DM实现一致性

VS的一致性协议：1.0基于DM实现一致性.png

• 把DM当成有推送能力的DB来用，一致性交给DM来解决
• 对DM读写太频繁，经常被限流

2.2 2.0版本山寨了Raft协议

VS的一致性协议：山寨了Raft协议.png

• term选主：确认leader和follower
1）主动发起选主：term+1
2）每次写入成功：term+100
3）term落盘
• 写入
1）Leader负责处理写入动作
2）Follower把写请求代理给Leader处理
3）写入成功：本地写入成功+n/2写入成功
4）Leader周期性的同步数据版本给follower

3、VS容灾：网络分区（Partition）

VS容灾：网络分区（Partition）.png

• 正常情况下，VS部署在et2和eu13机房，整个集群选出一个leader，通过raft和gossip协议达到多副本数据的一致性
• Partition期间，et2和eu13各自机房内部的调用不受影响，健康检查结果也是准确的
• Partition期间，VS集群读不受影响，但是写因为被隔离的小集群（eu13机器）因为没有leader而无法写，网络恢复后数据自动恢复一致性

4、VS服务发现模式：agent模式（DNS-F）

VS服务发现模式：agent模式（DNS-F）.png

• agent模式
• 客户主机端的独立进程
• 对客户端无侵入
• 拉取服务器中注册的全量域名
• 劫持本机所有DNS解析请求

DM
1、DM总览

DM总览.png

2、DM的一致性协议：基于DB的一致性协议

DM的一致性协议：基于DB的一致性协议.png

• 解决DB同server上的file cache一致性
• publisher随机注册到DM-server
• publisher同步完成注册（1~4）
• 水平通知集群其他所有的server本次『配置变更』（5）
• 从DB获取配置的最新值，更新本地缓存文件，更新内存（6）
• 3s内通知订阅端获取最新的配置（7）
• 补偿机制：server周期性同db全量检查配置一致性（8）

3、DM可用性：多级缓存

DM可用性：多级缓存.png

DM作为一个分布式环境下的持久配置系统，有一套完备的容灾机制，数据被存储在：数据库，服务端磁盘，客户端缓存目录
• 数据库主库不可用，可以切换到备库，DM继续提供服务
• 数据库主备库全部不可用，DM通过本地缓存可以继续提供读服务
• 数据库主备库全部不可用，DM服务端全部不可用，DM客户端使用缓存目录继续运行，支持离线启动