* 负载均衡

* 硬负载
** F5

** LVS
Linux Virtual Server也就是Linux虚拟服务器, 是一个由章文嵩博士发起的自由软件项目。
LVS由用户空间的ipvsadm和内核空间的IPVS组成，ipvsadm用来定义规则，IPVS利用ipvsadm定义的规则工作。
现在LVS已经是 Linux标准内核的一部分，在Linux2.4内核以前，使用LVS时必须要重新编译内核以支持LVS功能模块，
但是从Linux2.4内核以后，已经完全内置了LVS的各个功能模块，无需给内核打任何补丁，可以直接使用LVS提供的各种功能。

LVS的官方网站：http://www.linuxvirtualserver.org/
中文的资料主要在http://zh.linuxvirtualserver.org/handbooks ,
相比来说英文的资料会更全一些，http://www.linuxvirtualserver.org/Documents.html
FULLNAT的SYNPROXY的资料见http://kb.linuxvirtualserver.org/wiki/IPVS_FULLNAT_and_SYNPROXY

*** LVS特点
通过LVS提供的负载均衡技术和Linux操作系统实现一个高性能、高可用的服务器群集，它具有良好可靠性、可扩展性和可操作性。
从而以低廉的成本实现最优的服务性能。LVS的主要特点有以下几个方面：

高并发连接：LVS基于内核网络层面工作，有超强的承载能力和并发处理能力。单台LVS负载均衡器，可支持上万并发连接。稳定性强：是工作在网络4层之上仅作分发之用，这个特点也决定了它在负载均衡软件里的性能最强，稳定性最好，对内存和cpu资源消耗极低。
成本低廉：硬件负载均衡器少则十几万，多则几十万上百万，LVS只需一台服务器和就能免费部署使用，性价比极高。
配置简单：LVS配置非常简单，仅需几行命令即可完成配置，也可写成脚本进行管理。
支持多种算法：支持多种论调算法，可根据业务场景灵活调配进行使用
支持多种工作模型：可根据业务场景，使用不同的工作模式来解决生产环境请求处理问题。
应用范围广：因为LVS工作在4层，所以它几乎可以对所有应用做负载均衡，包括http、数据库、DNS、ftp服务等等
缺点：工作在4层，不支持7层规则修改，机制过于庞大，不适合小规模应用。

*** LVS核心概念
CIP: Client IP
VIP: Virtual IP
DIP: Director IP
RIP: Real Server IP

                                -->[rip]real server1
client[cip]-->[vip]director[dip]-->[rip]real server2
                                -->[rip]real server3

*** LVS三种工作模式
**** NAT(NetWork Address Translation - 网络地址转换)
    1. 集群中所有的节点必须在同一个网络中
    2. RIP是私有地址，仅用于集群节点之间的通信
    3. Director位于client和RealServer之间，负责处理进站与出站的所有请求
    4. RealServer必须将网关地址指向DIP
    5. 支持端口映射
    6. 较大规模的场景中，Director容易成为系统的瓶颈

NAT模式下，网络报的进出都要经过LVS的处理。LVS需要作为RS的网关。
当包到达LVS时，LVS做目标地址转换（DNAT），将目标IP改为RS的IP。RS接收到包以后，仿佛是客户端直接发给它的一样。
RS处理完，返回响应时，源IP是RS IP，目标IP是客户端的IP。
这时RS的包通过网关（LVS）中转，LVS会做源地址转换（SNAT），将包的源地址改为VIP，这样，这个包对客户端看起来就仿佛是LVS直接返回给它的。客户端无法感知到后端RS的存在。

**** DR
    1. 各集群节点跟Director必须在同一个物理网络中
    2. RIP可以是私有地址，也可以是公网地址
    3. Director仅负责入站请求，响应报文则直接由RealServer发往客户端
    4. 不支持端口映射
请求由LVS接受，由真实提供服务的服务器（RealServer, RS）直接返回给用户，返回的时候不经过LVS。
DR模式下需要LVS和绑定同一个VIP（RS通过将VIP绑定在loopback实现）。
一个请求过来时，LVS只需要将网络帧的MAC地址修改为某一台RS的MAC，该包就会被转发到相应的RS处理，注意此时的源IP和目标IP都没变，LVS只是做了一下移花接木。
RS收到LVS转发来的包，链路层发现MAC是自己的，到上面的网络层，发现IP也是自己的，于是这个包被合法地接受，RS感知不到前面有LVS的存在。
而当RS返回响应时，只要直接向源IP（即用户的IP）返回即可，不再经过LVS。
DR模式是性能最好的一种模式。
**** IP TUN
    模型与DR模型基本相同
    不同点在于：Director在转发请求的时候又对请求进行了一次封装，加了一个IP头部
    1. 集群节点可以跨越互联网
    2. RIP必须是公网地址
    3. DR仅处理入站请求，响应报文由RealServer发往客户端
    4. 只有支持隧道功能的OS才能用于RealServer
    5. 不支持端口映射
**** Full-NAT
无论是DR还是NAT模式，不可避免的都有一个问题：LVS和RS必须在同一个VLAN下，否则LVS无法作为RS的网关。
这引发的两个问题是：
1、同一个VLAN的限制导致运维不方便，跨VLAN的RS无法接入。
2、LVS的水平扩展受到制约。当RS水平扩容时，总有一天其上的单点LVS会成为瓶颈。
Full-NAT由此而生，解决的是LVS和RS跨VLAN的问题，而跨VLAN问题解决后，LVS和RS不再存在VLAN上的从属关系，可以做到多个LVS对应多个RS，解决水平扩容的问题。

在包从LVS转到RS的过程中，源地址从客户端IP被替换成了LVS的内网IP。
内网IP之间可以通过多个交换机跨VLAN通信。
当RS处理完接受到的包，返回时，会将这个包返回给LVS的内网IP，这一步也不受限于VLAN。
LVS收到包后，在NAT模式修改源地址的基础上，再把RS发来的包中的目标地址从LVS内网IP改为客户端的IP。
Full-NAT主要的思想是把网关和其下机器的通信，改为了普通的网络通信，从而解决了跨VLAN的问题。采用这种方式，LVS和RS的部署在VLAN上将不再有任何限制，大大提高了运维部署的便利性。

*** LVS负载均衡算法
1. 静态调度
RR （Round Robin）-- 轮询
将请求按照顺序轮流分配到集群中的真是服务器上，它均等地对待每一台服务器而不管服务器上实际的连接数和系统负载。
WRR （Weighted Round Robin）-- 加权轮询
根据真是服务器的不同处理能力来调度访问请求。这样可以保证处理能力强的服务器处理更多的的访问量。调度器可以自动问询真是服务器的负载情况，并动态地调整其权值。
SH（Source Hashing) -- 源地址Hash
根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。
DH（Destination Hashing）-- 目的地址Hash
根据请求的目标IP地址，作为散列键值（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用且未超载，将请求发送到该服务器，否则返回空。

2. 动态调度
active -- 活动连接数
inactive -- 非活动连接数

LC（Least Connections）-- 最少连接数
active * 256 + inactive
WLC（Weighted Least Connections) -- 加权最少连接数
(active * 256 + inactive)/weight
SED（Shortest Expected Delay） -- 最短期望延迟
（active + 1）* 256 / weight
NQ（Never Queuen）永不排队
该调度算法是在SED是由改进的SED而来，因为在SED中可能会存在权重小的机器终都得不到Director转发的请求。因此为了让转发更合理，而出现了NQ。
LBLC（Locality-Based Least Connections） -- 基于本地的最少连接
针对目标IP地址的负载均衡，主要用于Cache集群系统中。Director维护一个IP到一台服务器的映射。
LBLCR（Locality-Based Least Connections With Replication） -- 带复制的基于本地的最少连接
基于LBLC优化，也是主要用于Cache集群系统中。Director维护一个IP到一组服务器的映射。

*** LVS高可用
容灾分为RS的容灾和LVS的容灾。
RS的容灾可以通过LVS定期健康检测实现，如果某台RS失去心跳，则认为其已经下线，不会在转发到该RS上。
LVS的容灾可以通过主备+心跳的方式实现。主LVS失去心跳后，备LVS可以作为热备立即替换。
容灾主要是靠KeepAlived来做的。

*** LVS问题
如果过LB的请求量就大到把LB给打挂了怎么办?互联网的流量，尤其是中国互联网的流量，我们要有足够的自信啊，而且参与过春节买票的，春晚修一修抢红包的都能想象得到。
LB虽然可以有standby的方案或者有小规模集群能力，但如果active/standby同时挂了怎么办? 1个蛋蛋很危险，但2个蛋蛋也未必就多安全。比如在active-standby方案中，既然active撑不住请求流量，那么作为其clone的standby身上当然也不会出现任何奇迹，那么是不是LB前面还应该再架一层LB呢?能不能LB集群全挂了的情况下，不影响正常的业务?
请求方和目标机器之间总是要过一次LB,这在网络链路上是多了1跳，我们都知道多一跳可不光是rt的损耗那么简单，链路上从1跳到2跳，链路和连接出故障的概率也翻了一倍,这要怎么解？
多机房，多区域的异地多活与容灾，国际化战略的跨国流量的容灾对于负载均衡提出的挑战怎么解，在阿里集团内部，现在断网、断电、断机房的演习如日常喝水、像办公大楼消防演习一样随意，据说要达到，马老师半夜起来上个厕所，顺便断个电的能力，这些容灾场景下业务流量的负载均衡怎么解？
每次在一些如“秒杀”，“大促”等营销热点场景下，业务为了应对可以预期的流量洪峰，评估LB这一块的容量够不够、要扩多少的痛点又如何解决?LB的弹性在哪里?
成本。虽然LVS比一些传统硬件LB的成本已经有很大的优势，但是在一个大型互联网系统级别的流量和业务发展面前,LVS的使用成本还是太高了一点。

*** 安全
**** SYNPROXY 技术概述
LVS 针对 TCP 标志位 DDOS 攻击，采取如下策略：
1.对于 SYN flood 类型攻击，利用 SYNPROXY 模块进行防御。
如下图所示，主要实现方式为：参照 Linux TCP 协议栈中 SYN cookie 的思想，LVS 代理 TCP 三次握手。代理过程：
Client 发送 SYN 包给 LVS。
LVS 构造特殊 SEQ 的 SYN ACK 包给 Client。
Client 回复 ACK 给 LVS。
LVS 验证 ACK 包中 ack_seq 是否合法。
如果合法，则 LVS 再和 Realserver 建立 3 次握手。

2. 对于 ACK/FIN/RSTFlood 类型攻击，查找连接表，如果不存在，则直接丢弃。

*** 四层负载和七层负载
四层与七层的简单对比：
四层的转发能力强
七层更因为工作在应用层，因此更灵活（动静分离/URL重写等）

* 负载均衡演进
** 负载均衡策略
纵向扩展（Scale Up)
没啥好说的，也许只是服务器还不够NB，买！买！买！宇宙最好的服务器来上一台，可惜，创业刚开始，投资人的钱要花在刀刃上，比如广告营销上，比如路边扫个码啊，顺便送个内衣啊、牙刷啊什么的，服务器? 买不起! 而且看了一下宇宙上最好服务器的网卡配置，更泄气，这就不是有钱买个NB服务器就能解决的事！
业务拆分
你仔细审视之后，发现其实你的系统的2个页面是2个不同的业务，用来满足不同的需求的，于是啊，你就想，
是不是能把这2个业务分开到2个系统中去，这样用户们自然乖乖的被引导、分流到2个子系统去了，这样每个系统的压力就减少了啊。
这就好比一个饭店，生意火爆，原先来吃火锅和吃大饼卷牛排的都在一起，挤不下之后，现在分成了2个店，1个是火锅店，1个是大饼卷牛排店。
从某种角度来说，这种其实也是一种负载均衡，但怎么做业务拆分并且通过组织文化和人事架构去保障和适应这种业务拆分，
是有很多的业务考量和业务属性在里面的，每个老板和每个行业的答案可能都是不同的，此文讨论的不在这个方向上。

横向扩展（Scale Out）和 副本(Replica)
做完上面的垂直拆分之后，可能你会发现还是不行啊，这世界上拥有独特口味的人太多，吃大饼卷牛排的人还是太多了，怎么办?
开分店吧，每个上档次的商场都开一个大饼卷牛排店，这样每个地域的人又被分流到了附近的店。
这个提供一模一样服务的“分店”就是系统的副本（Replica），分布式系统中的副本(Replica)除了满足数据冗余，
容灾的需要等之外，横向扩展，通过开多个分店（Replica)分流的行为,就是负载均衡,做到多副本之间分流是一个重要的目的。

1. DNS负载均衡
一个域名，我们可以添加多个A记录，绑定后端多台服务器（如果没有SLB的话），这样我们可以利用DNS的负载均衡帮我们实现服务器断的负载均衡。
在无负载均衡的权威 DNS 中，Local DNS 访问权威 DNS，权威 DNS 会将这 绑定的多个解析记录全部返回给 Local DNS，
 Local DNS 会将所有的 IP 地址返回给网站访问者，网站访问者的浏览器会随机访问其中一个 IP。
而在有负载均衡的权威 DNS 中，网站访问者的请求到来时，权威 DNS 会根据解析记录的权重轮询 全部 A 记录(默认权重 1:1:1)，依次返回 3 个 IP 地址，
每次返回一个IP地址；当然用户可以在这里修改A记录的权重值。
问题：
1. 会话粘连
客户端与目标系统之间一般存在会话的概念（不止是web系统的http session）, 其本质在于server端会或多或少的存一些客户端整个会话期间交互的身份识别以及数据信息，
为了防止server端每次都对同一个客户端问一下，你是谁？系统会希望客户端在一个会话期间粘连在某个特定的serer上，除非这个server失败才failover到其它的server上，
这种粘连特性对于server处理客户端请求处理的性能和客户端看到的数据一致性是有很大好处的。但是DNS负载均衡不能保证下一次请求会再次落在同一个server上。
2. TTL
使用DNS负载均衡的问题是一般DNS解析都会有TTL(TTL指各地DNS缓存您域名记录信息的时间),
当后端某个服务器挂掉的时候，由于TTL的缓存得不到及时清除，所以会让部分流量进入到已经宕掉的机器上，造成一定的损失。
所以，这个时候，我们最好引入相比于后端应用更加稳定、相比于DNS负载均衡更加灵活的负载均衡器，前置在我们的链路中，比如LVS、F5或者Nginx。
3. 热点/容错

2. 引入LVS
我们采用官方的LVS作为软负载，并通过主备的方式达到LVS的容灾策略，LVS与Real Server之间通过heartbeat方式进行健康检查，
LVS主备间通过KeepAlived进行状态检测；通过LVS，当我们Real Server 集群有机器上下线时，就不需要与DNS打交道了，
只需要与LVS交互，并且LVS本身可具备对RealServer的健康检查，让我们的服务上下线变得更加容易和简单，并且我们可以在LVS中自定义相比于DNS更丰富的负载均衡策略；
这种架构中，同一时刻只有一台LVS对外提供服务，另外一台一直处于stand by 状态，直到提供服务LVS的挂了。
在LVS中配置了Real Server的真实IP地址，用于健康检查和负载均衡，在Real Server中配置了Virtual IP地址，用于组件Virtual Server环境（LVS + Real Server统称为Virtual Server）。
我们可以看到Virtual IP 并不像内网或者外网IP那样绑定上去后就固定唯一了，Virtual IP 我们是可以绑定到多台机器上。
即使我们LVS采用了主备模式，单点提供服务的LVS还是可能会成为性能瓶颈，无法进行横向扩张，其次，官方LVS缺乏攻击防御功能，在转发模式上，只支持NAT/DR/TUNNEL 三种，在多VLAN 网络环境下部署成本极高。
那么如何解决这些问题呢？阿里的负载均衡设备在官方 LVS 基础上进行了定制化和优化，比如LVS采用集群方式部署，增加攻击防御模块，新增转发模式 FULLNAT，实现 LVS-RealServer 间跨 VLAN 通讯等；
Ali-LVS 的开源地址： https://github.com/alibaba/LVS

3. 改进版负载均衡
我们的LVS变成了集群模式，那么就需要我们感知LVS集群中的机器服务状态并能自动进行健康检查，
LVS本身也需要负载均衡；这个时候就需要引入另外一个硬件设备了-交换机，LVS和交换机间运行OSPF心跳，
1个VIP（Virtual IP）配置在集群的所有LVS上，ECMP负责将数据流分发给LVS集群，当一台LVS宕机后，交换机会自动发现并将其从ECMP等价路由中剔除。
外部流量到了交换机后下一步具体走哪条路径是在交换机上配置不同的hash策略控制的，一般是源IP+源端口。


4. 从4层负载到7层负载
当然，这种架构已经能解决我们大部分问题了，但是LVS是在四层协议上实现的负载均衡，
我们有一些业务需要SLB实例服务端口使用的是7层HTTP协议，怎么办呢？
我们可以引入Tengine，Tengine是当前最流行的7层负载均衡开源软件之一，且已经开源，那么我们的架构可能会演变成以下这个样子：
客户端访问SLB实例VIP时，相关请求由SLB实例对应的LVS集群处理，如果相应的SLB实例服务端口使用的是4层协议（TCP或UDP），
那么LVS集群内每个节点都会根据SLB实例负载均衡策略，将其承载的服务请求按策略直接分发到后端Real Server服务器，并同时维护会话保持等特性；
如果相应的SLB实例服务端口使用的是7层HTTP协议，那么LVS集群内每个节点会先将其承载的服务请求均分到Tengine集群；
而后，Tengine集群内的每个节点再根据SLB实例负载均衡策略，将服务请求按策略最终分发到后端Real Server服务器，并同时维护会话保持等特性。

4. 从公网访问负载到内部访问负载
我们通过LVS集群和Tengine集群解决了公网到阿里内部应用的负载均衡，随着阿里内部很多应用的集群逐渐扩大，
这些应用在被访问的时候也是需要被进行负载均衡（我们默认已包含状态检查模块）的，那么我们把应用访问方式大致分为两类，
一类是基于HSF的RPC远程服务调用，另外一种是基于HTTP的服务调用；基于HSF的服务调用的负载均衡是由ConfigServer完成，
这部分可以参考ConfigServer的实现；基于HTTP的负载均衡是VS，通过VS，我们内部应用通过域名范围内部应用时，
就没有必要重新走一遍公网的DNS解析，链路顶端的各种负载均衡了，通过VS，我们可以与内部应用进行直连，
相当于对我们的访问链路进行了优化和加速，当然，VS在对HTTP的内部调用上除了可以做负载均衡、健康检查外，
还做了异地容灾的调用实现、流量的控制、环境管理、灰度的发布等，有关VS的功能可以参考VS的功能介绍。

5. ADNS和Aserver

** ADNS
ADNS是集团自研的高性能权威DNS。ADNS提供了线路智能解析、机房自动/手动灾备切换、分机房流量调度等功能。
 其主要原理如下：
先将业务域名CNAME至gds域名，例如a.taobao.com CNAME 至a.taobao.com.gds.alibabadns.com，
再为对应的gds域名a.taobao.com.gds.alibabadns.com分配对应的A解析IP地址。
从系统稳定性角度来看，个人觉得ADNS提供给业务最重要的价值就是机房灾备切换和机房级别的流量调度。
为了提高服务的可用性，业务系统一般采用多机房部署架构，当一个机房的服务不可用时，可通过ADNS将流量切换到另一个机房，从而使业务系统继续提供服务。

** AServer
随着集团全网HTTPS项目的开展，所有前端应用的请求协议逐渐由HTTP转换成HTTPS。
HTTPS涉及到SSL证书，之前的SSL证书以.key和.crt的文件的方式存放在Tengine的配置里。
而SSL证书每隔若干年就要更新一次，如果采取以前应用流量分流到每个应用单独的VIP的方式，
将带来海量证书过期的不可维护性。因此集团决定统一HTTPS入口，也就是现在的统一接入层（AServer）。

统一接入层是一个Tengine运行的Web Server代理，由它负责承担全网用户的HTTP/HTTPS请求，
然后将请求转发给后端的应用服务器。统一接入层也称Aserver。

上图是系统接入AServer以后的集团网络架构。可以看到，系统接入AServer之前，外部请求是直接通过LVS集群转发给服务器集群。
接入AServer之后，HTTP/HTTPS请求先通过LVS集群转发到AServer，然后再由AServer转发给业务集群。
图中右侧的VS用于帮助AServer将请求正确地路由到对应的业务集群，AServer与VS之间的交互如下：

AServer从请求头中获取请求的域名；
根据域名找到对应的VS Key；
向VS发送域名对应的VS Key；
VS返回Key对应的后端业务集群的Real IP 列表；
AServer选择IP进行转发；
VS 会定期对后端业务集群做健康检查，更新 IP列表。
和基于LVS方式的负载均衡方式相比，VS通过集中式的配置向客户提供路由信息，以非网关的形式实现了负载均衡功能。
