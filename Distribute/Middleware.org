* 分布式
** 定义
定义：分布式是网络连接的计算机系统，每个节点独立承担计算或存储服务，节点间通过网络协同工作。
本质：分布式的出现本质上是为了使用更多的机器完成单个计算机节点无法完成的计算或存储任务。

** 理解
*** 本质
计算/存储/网络
空间/时间
水平(横向)/垂直(纵向)
分拆(Map)/合并(Reduce)

*** 演进
单体应用>SOA>微服务
** 理论
*** 时间和顺序
因果律、NTP、逻辑时钟、向量时钟、原子钟
*** 理论
ACID
BASE
CAP
*** 一致性算法
PAXOS、RAFT、GOSSIP

** 高性能
*** 业务视角
*** 集群视角
*** 单机视角
*** 数据视角

** 高可用

*** 视角
1.时空视角：时间(性能、超时、重试、熔断、限流、SLA)，空间(分流/负载均衡、单点/分区、数据一致性、强弱依赖/降级、隔离、热点、容灾、故障转移)
2.纵横视角
3.分和视角
4.角色分工视角：架构/稳定性(服务治理)/容量/运维/安全
5.历史演进视角

*** 架构视角
1. 单点/分区
从请求发起侧到服务处理返回的调用全链路的各个环节上避免存在单点（某个环节只由单个服务器完成功能），
做到每个环节使用相互独立的多台服务器进行分布式处理，要针对不同稳定性要求级别和成本能力做到不同服务器规模分布式，
这样就避免单个服务器挂掉引发单点故障后进而导致服务整体挂掉的风险。
可能涉及的环节有端动态获取资源服务(html& js &小程序包等)、域名解析、多服务商多区域多机房IP入口、
静态资源服务、接入路由层、服务逻辑层、任务调度执行层、依赖的微服务、数据库、消息中间件。
消除单点可能策略：在服务逻辑层采用多运营商多IP入口、跨地&同地多机房部署、同机房多机器部署、
分布式任务调度等策略；在数据存储层采用数据库分库分表、数据库主从备集群、
KV存储&消息等分布式系统集群多副本等策略。
有分布式处理能力后同步需要考虑单个服务器故障后自动探活摘除、服务器增删能不停服自动同步给依赖方等问题，
这里就需引入一些分布式中枢控制系统，如服务注册发现系统、配置变更系统等，
例如zookeeper是一个经典应用于该场景的一个分布式组件。
2. 数据一致性
在分布式处理以及微服务化后，相关联的数据会存在于不同的系统之中，相关联的数据库表、数据存储、缓存等数据会因为架构设计或子系统抖动故障失败等原因导致彼此数据出现不一致，
这也是一类稳定性故障。最简单一致性问题就是关系型数据库的同请求内同库相关联多个数据表更新的一致性，
这个可通过数据库的事务以及不同事务级别来保证。从架构层面，数据一致性也会根据业务特点，做强一致性、
最终一致性的架构选型，不同选型根据CAP理论，也会带来可用性以及分区容忍性的一些折衷。
在做好SLA高的基础系统选型、分布式事务中间件使用、幂等设计、针对性提升好微服务本身SLA后，
可根据不同数据一致性级别要求，考虑通过消息触发多系统对账、定时调度对账、子流程失败后主动投递消息延迟重试、
消息消费失败后回旋重试、数据库记录过程中状态后做定时调度扫描未成功记录后重试、离线全量对账。
缓存更新机制不合理也容易引发缓存和数据库之间数据不一致，一般在数据更新时考虑并发更新时缓存删除优先
 或 固定单线程串行更新策略。复杂分布式业务系统 或微服务化治理后，基于消息中间件的解耦是普遍方式，
 这时选择一个确保自身不重不丢以及高SLA消息中间件非常重要，利用消息中间件自身的多次回旋重试基本能保障很高的最终一致性，
 数据一致性要求更高的，再配合不同级别对账机制即可。

4. 强弱依赖/降级熔断
5. 热点或者极限值处理
6. 资损防控
7. 离线数据

*** 服务治理视角
1. 负载均衡
2. 分流
分流能力也是服务化框架的一个非常重要的能力。总结而言，分流就是通过路由规则，将部分请求分发到特定的服务子集，达到进程间隔离的效果。
分流在实际的业务场景中使用非常广泛，常见的有：
根据服务的方法来做分流，可实现快方法/慢方法/读方法/写方法的流量分离
根据Caller上游域来分流，可实现在线服务的请求/离线服务的请求/定时任务的请求的流量分离
根据Caller上游域来分流，可实现核心/非核心业务的请求流量分离
在技术手段上，要支持一个灵活的分流功能，需要服务化框架能够从报文头或者指定的内存对象中解析到分流参数并根据规则在运行时映射到对应的服务集中。
同时为提供维护效率，在平台配置的角度，需要提供灵活的DSL和可视化配置界面。另外如果服务化框架上K8S并且采用动态IP策略（即每次容器发布完后IP都会更新），
那么分流规则需要支持K8s的Label能力，根据Label动态的读取打标的服务集
3. 超时->进阶能力：超时时间传递
超时：为避免集群雪崩或资源耗尽，一切调用都应该设置超时时间，包括服务调用/DB/缓存，服务端超时是必选配置。
服务端需要提供默认超时设置，各业务可按需配置个性化超时时间。上游客户端有时为了自己保命（有一句经典老花：不怕死只怕慢），
也会按需设计超时。具体的超时控制的粒度，要支持到服务级别/方法级别/客户端调用级别

根据实际的电商实际场景中，一个服务级别的超时时间通常会设置在100ms～300ms之间。
而入口层应用，由于承担了所有的流量，对延时相对苛刻（在没有隔离的情况下，一旦某个下游调用慢，将可能拖垮整个应用），
其客户端调用下游服务时，会将客户端超时设置到一个比较小的值，例如10ms～50ms之间

超时时间传递：
在一个极度复杂的网状的微服务体系下超时时间的设定其实是一件很头疼的事情。假设我们有三个业务域，A->B->C,
 如果A域的服务超时时间设置为100ms，B域为200ms，C域为100ms。如果A域访问完一个DB后，已经耗时50ms，
 那么这个请求剩余的“存活时间”仅有50ms，当这个请求到达B之后，在B的服务逻辑中处理了60ms，其实这个请求在A端已经超时返回，
 那么它继续调用C已经毫无意义，对服务资源是一种浪费。这里也引出了我上面说的“超时时间传递”这个能力

所谓“超时时间传递”是指服务框架会综合判断上游的超时时间/本层的调用时间/实际已流逝的时间，将剩下的可用时间往下游透传。
下游服务可判断该次请求整体是否已经超时，如超时则及时中断后续请求的执行。超时时间传递能力除了能够减少服务开销，也可以极大的降低下游业务雪崩的风险
4. 重试->进阶能力：全局重试
重试是指当服务调用发生超时错误或者从服务端返回一个“可重试”的异常时，选择另外一个后端服务实例进行转发。可见重试可以在服务端的偶发性抖动或者网络质量出现抖动的情况下，来提升服务SLA
但时重试也是一个风险非常大的功能，首先重试意味的后端服务必须具有幂等操作能力，其次如果后端服务处在一个即将产生故障的临界点（坡脚鸭状态），
重试将会让后端服务雪上加霜，极大增加了雪崩的风险。这也是大部分电商级的服务框架，默认情况下，请求是不做重试的
全局重试：
重试是把双刃剑，用好了能提升服务SLA，用不好则极大概率会导致服务的级联雪崩。
那么我们有什么更好的策略能享受重试带来的红利的同时来降低重试的风险呢？答案是有的，非常简单，我们可以通过设定全局重试阈值来控制在窗口T内的总的重试阈值。
例如在一个HSF客户端X，对于下游服务Y的在1s窗口内最多允许的超时个数是100次，一旦重试超过100次，则及时中断。这样既兼顾了下游的抖动又避免了大面积重试的风险
5. 隔离->进阶能力：动态隔离
服务隔离是将服务在进程级别或者线程级别做物理或逻辑隔离，避免由于某些服务方法异常导致服务整体不可用。可以说隔离是微服务体系中非常非常重要的能力，对阿里中台这种混合业务平台尤为关键。
服务进程级别的隔离，其实是通过上面提到“分流”策略来实现。通过将某个大服务集按需拆分成多个小的服务组，不同的上游客户端根据策略将流量引到这些物理的小的服务集中去。
进程级别隔离成本和运维的代价都很高，需要合理控制服务组的数量和流控规则的复杂度，不然到后面绝对是你的噩梦
线程级别隔离：相比进程隔离，线程级别隔离显的更加轻量级，维护代价更低。线程隔离可以分为静态配置隔离和动态自适应隔离。
静态配置隔离通过手动为每个服务/方法设置工作线程池的数量来实现，它的优点是简单和确定性，缺点是配置繁琐且对变化的适应性很差。在实际的落地配置时，一般只会配置核心接口/方法的线程池，无法面面俱到
动态隔离：
相比静态隔离需要手工维护，动态隔离则明显更高大上。动态隔离一般有二种策略，第一种是可以计算每一个接口/方法的访问延时，
一旦发现在N个窗口内接口/方法延时超标，则自动将它们隔离到独立的线程池中，避免影响其它调用。第二种更轻量化，初始状态所有的服务请求还是共用一个业务线程池，
一旦业务线程池耗尽，系统开始为每一个服务接口/方法分配独立的线程池，同时这些线程池都设置好Idle时间，ThreadPool的corePoolSize都设置为0，待共享业务线程池恢复后，
请求回归到共享业务线程池。之前独立分配的方法线程池也会随时间的推移逐步Idle掉。当然这里也可以设置一些策略，比如共享业务线程池耗尽后，先Fail-Fast，不立即创建独立线程池。等待T个周期还不恢复，再进入隔离流程
6. 熔断->进阶能力：子集划分
熔断其实是服务化体系中端到端的自治能力的体现。如果一个客户端调用某个服务实例的接口方法出现异常导致一个比例时（异常比例=窗口内异常数/窗口内总请求数），
就站在这个客户端角度，把这条调用链路从服务列表中暂时屏蔽掉，客户端访问服务列表中的其它实例。但这个实例的其他方法依然可达，
同时其他客户端依然对这个接口方法的请求依然可达。当这条链路熔断后，会有同小流量探测这条链路的可达性，如果判定可达，则将这条链路恢复。
可见熔断的作用范围细化到请求链路级别，这也是熔断最大的价值。通过端到端链路级别的控制，可以避免某个方法导致后端服务整体不可用，同时，客户端对下游服务质量的响应也是最及时和准确的。
子集划分：
子集划分是指每个客户端以随机或者确定性算法选择下游服务实例列表中的subsettting作为自己的服务方
子集划分的意义：一个客户端调用下游服务端实例时需要维护长连接资源，当下游服务端实例数大到一定规模时（例如上万个实例），维护长连接将消耗可观的Memory和CPU资源。通过子集划分可降低长连接的overhead
当下游服务实例数过多时，熔断计算量也是一笔不小的开销。通过子集划分可降低熔断的计算范围
7. 限流->进阶能力：动态限流
限流是保护服务资源的常规手段，通过限制请求QPS/线程数/并法连接数来限制客户端对服务资源的访问。限流试图通过一组确定性的阈值来增加服务行为的确定性，而这个阈值，通常是通过单机压测和全链路压测来获取。
限流在大促秒杀场景下显的非常关键，在零点时刻，突然增长的亿万用户请求洪峰蜂拥而至，而一个确定性的限流阈值，能够给予系统更多的确定性和信心，哪怕这个阈值会损失部分正常的业务流量，对大促场景也是可以接受的。
限流分为单机（单实例）限流和集群限流。一般单机限流是最常用的，集群限流在入口层比较常用，需要依赖中央计数器，例如依赖Redis做中心计数。
动态限流：
限流落地最令人头疼的还是在限流阈值的设置上，在阿里这类电商级服务体系下，服务方法/上下游客户端数量众多，
业务场景变化很大（业务场景的变化，也意味着流量模型的变化）。如果需要手工来维护限流阈值，去设置QPS/线程数/并法数，
成本代价还是很大。这个从业务侧同学的反馈上也是这个观点：这么多接口和方法，基本凭经验和拍脑袋，新接口上线/老接口下线，阈值都来不及更新的
为避免手工维护限流阈值，动态限流是一个常规手段，所谓动态限流，就是收集系统的RT/CPU Usage/CPU Load/线程数等系统信息，
当接近或达到红线阈值时，直接拒绝请求或根据请求优先级来有条件的拒绝请求。这个手段在一定程度内可以解决问题。
但大家思考一下像双11大促这样的场景，秒级流量会暴增百倍以上，动态限流的计算实效性，无法支撑这么快的暴增流量，
可能这1秒内，系统就会处于坡脚鸭状态，而从坡脚鸭状态恢复到正常态，是需要一定的时间，这个在大促态是非常大的风险。
所以大促态时，常规策略是宁可相信固定阈值限流也不会采纳动态限流，而动态限流则作为兜底的保护机制

** 可伸缩

** 扩展性

** 安全性

** 稳定性
Google SRE中(SRE三部曲)有一个层级模型来描述系统可靠性基础和高层次需求(Dickerson's Hierarchy of Service Reliability)
该模型由Google SRE工程师Mikey Dickerson在2013年提出，将系统稳定性需求按照基础程度进行了不同层次的体系化区分，形成稳定性标准金字塔模型。
金字塔的底座是监控(Monitoring)，这是一个系统对于稳定性最基础的要求，缺少监控的系统，如同蒙上眼睛狂奔的野马，无从谈及可控性，更遑论稳定性。更上层是应急响应(Incident Response)，从一个问题被监控发现到最终解决，这期间的耗时直接取决于应急响应机制的成熟度。合理的应急策略能保证当故障发生时，所有问题能得到有序且妥善的处理，而不是慌乱成一锅粥。事后总结以及根因分析(Postmortem&Root Caue Analysis)，即我们平时谈到的“复盘”，虽然很多人都不太喜欢这项活动，但是不得不承认这是避免我们下次犯同样错误的最有效手段，只有当摸清故障的根因以及对应的缺陷，我们才能对症下药，合理进行规避。
假设一个系统从初次发布后就不再进行更新迭代，做好上述三个方面的工作就能基本满足系统对于稳定性的全部需求。可惜目前基本不会存在这样的系统，大大小小的应用都离不开不断的变更与发布，因此要保证系统在这些迭代中持续稳定，测试和发布管控(Testing&Release procedures)是必不可少的。有效的测试与发布策略能保障系统所有新增变量都处于可控稳定区间内，从而达到整体服务终态稳定。除了代码逻辑更新，迭代同样可能带来业务规模及流量的变化，容量规划(Capacity Planning)则是针对于这方面变化进行的保障策略。现有系统体量是否足够支撑新的流量需求，整体链路上是否存在不对等的薄弱节点，都是容量规划需要考虑的问题。
位于金字塔模型最顶端的是产品设计(Product)与软件研发(Development)，即通过优秀的产品设计与软件设计使系统具备更高的可靠性，构建高可用产品架构体系，从而提升用户体验。