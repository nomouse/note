* 概念
  后端系统无法满足访问量的增加带来的流量和处理压力，本质上是分而治之的思想
** 解决方案1:纵向扩展(Scale Up)
   提升机器性能，网卡性能，程序性能，等等
** 解决方案2:横向扩展(Scale Out)和副本(Replica)
   部署一个系统的多个副本，既可以满足数据冗余容灾，又可以分流压力
** 解决方案3:业务拆分
   把一个系统按照业务拆成多个系统，比如登录、购买、支付
* 常用负载均衡技术
** DNS轮询
DNS本身的机制不再赘述，这里主要看一看基于DNS的负载均衡，其大致原理很清楚，DNS系统本身支持同一个域名映射到多个ip (A记录）,例如

niubility.com.      IN     A    172.168.1.101
                    IN     A    172.168.1.102
                    IN     A    172.168.1.103
                    IN     A    172.168.1.104
这样每次向DNS系统询问该域名的ip地址时（Tell Me The IP Address of niubility.com.），DNS会轮询(Round Robin)这个ip列表，每次给一个不同的ip，从而达到负载均衡的效果。

来看看这种负载均衡解决方案的优缺点

优点

易于实现
对于应用系统本身几乎没有任何侵入，配置也很简单，在某个文本里多加几行A记录就可以。尤其对于一个基于Web的系统来说更是如此，用户在浏览器里输入的URL host部分天然就是域名，所以在某个环节你必然有起码一台DNS服务器记录着这个域名对应的ip，所以可以说是基于已有域名系统(资产)就做到了负载均衡。

缺点

会话粘连 (Session Sticky)
客户端与目标系统之间一般存在会话的概念（不止是web系统的http session）, 其本质在于server端会或多或少的存一些客户端整个会话期间交互的身份识别以及数据信息，为了防止server端每次都对同一个客户端问一下，你是谁？系统会希望客户端在一个会话期间粘连在某个特定的serer上，除非这个server失败才failover到其它的server上，这种粘连特性对于server处理客户端请求处理的性能和客户端看到的数据一致性是有很大好处的。但是DNS负载均衡不能保证下一次请求会再次落在同一个server上。

DNS解析缓存和TTL带来的麻烦
dns记录的缓存以及缓存失效时间都是个问题，在无线时代，通常来自手机的访问会经过称为行业网关的代理服务器，由于代理服务器会将域名解析的结果缓存一段时间，所以所有经由这个代理服务器的访问请求就全被解析到同一台服务器上去了，因此就可能无法实现均等分配需要处理的请求了。另外在后端集群的拓扑结构(副本数、部署位置、健康状态等)发生变化之后，dns配置的变化要等到网络上所有节点的缓存失效才能反馈出来，这带来的问题起码有2个，1是在等待失效过程中，完全不可控，没有办法加快这个进程，中美切换要花10分钟，因为要等网络所有几点对某些域名的TTL失效，2是滞后，有时候这种滞后是致命的，比如仍然有部分流量打到已经挂掉的那部分服务器上。

容错
一个大型数据中心，每天都有机器坏了是很正常的事情，尤其是在虚拟化大行其道的今天，更是如此，相信你对虚拟主机又崩溃了一个，或者总是被同宿主机的猪一样的队友“挤”死这种情况一定不陌生。dns负载均衡的一大问题就在于这种情况下的容灾很麻烦，一是需要人工干预或者其他软件配合做健康监测，从dns配置中将无响应的机器或者崩溃的机器的相应的A记录删掉。一是删掉之后也要等到所有网络节点上的dns解析缓存失效，在这端时间内，很多访问系统的客户会受到影响。

数据热点
dns是在域名层面做负载均衡，如果从web系统的请求URL角度讲，不同的URL对后端server的压力强度不一样，dns负载很可能会出现所有高强度的请求全都被打到小部分服务器甚至同一台上去了的情况，这个问题的可怕性不在于风险，而在于风险完全不可控。

** 负载均衡器
LB负责客户端流量到后端服务集群的分发，一般LB也会负责后端所有server的健康监测，关于健康监测这一块我们在稍后一点再做具体分析。

优点

可以在LB里面做集中的分发逻辑，可以有多种分发方式，例如最常见的Round Robin, Random Robin, Weight-Based Round Robin等。

缺点

LB是单点, 这里其实是有2个问题，下面具体分解一下。

问题1: 所有流量(请求流、响应流）都要经过LB，量太大，LB这小身板扛不住啊，尤其是响应流，一般远大于请求流，为什么? 我们想想一般一个http请求报文大小和响应的报文大小的对比就明白了。
现在一些LB也会支持Active-Active模式，这里不再介绍。

四层LB vs 七层LB
四层LB的特点一般是在网络和网络传输层(TCP/IP)做负载均衡，而七层则是指在应用层做负载均衡。2者的区别还是比较大的，各有优缺点，四层LB对于应用侵入比较小，这一层的LB对应用的感知较少，同时应用接入基本不需要针对LB做任何的代码改造。七层负载均衡一般对应用本身的感知比较多，可以结合一些通用的业务流量负载逻辑和容灾逻辑做成很细致的负载均衡和流量导向方案，但是一般接入时，应用需要配合做相应的改造。

互联网时代流量就是钱啊，对于流量的调度的细致程度往往是四层LB难以满足的，可以说七层负载均衡的解决方案现在是百花齐放，百家争鸣，中间层负载均衡(mid-tier load balancing)正当其时。
** 遗留问题
如果过LB的请求量就大到把LB给打挂了怎么办?互联网的流量，尤其是中国互联网的流量，我们要有足够的自信啊，而且参与过春节买票的，春晚修一修抢红包的都能想象得到。
LB虽然可以有standby的方案或者有小规模集群能力，但如果active/standby同时挂了怎么办? 1个蛋蛋很危险，但2个蛋蛋也未必就多安全。比如在active-standby方案中，既然active撑不住请求流量，那么作为其clone的standby身上当然也不会出现任何奇迹，那么是不是LB前面还应该再架一层LB呢?能不能LB集群全挂了的情况下，不影响正常的业务?
请求方和目标机器之间总是要过一次LB,这在网络链路上是多了1跳，我们都知道多一跳可不光是rt的损耗那么简单，链路上从1跳到2跳，链路和连接出故障的概率也翻了一倍,这要怎么解？
多机房，多区域的异地多活与容灾，国际化战略的跨国流量的容灾对于负载均衡提出的挑战怎么解，在阿里集团内部，现在断网、断电、断机房的演习如日常喝水、像办公大楼消防演习一样随意，据说要达到，马老师半夜起来上个厕所，顺便断个电的能力，这些容灾场景下业务流量的负载均衡怎么解？
每次在一些如“秒杀”，“大促”等营销热点场景下，业务为了应对可以预期的流量洪峰，评估LB这一块的容量够不够、要扩多少的痛点又如何解决?LB的弹性在哪里?
成本。虽然LVS比一些传统硬件LB的成本已经有很大的优势，但是在一个大型互联网系统级别的流量和业务发展面前,LVS的使用成本还是太高了一点。
