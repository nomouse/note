* 硬负载
*

* 软负载
** VIPServer

VIPServer : 一种集群路由软件负载均衡系统
VIPServer: A Cluster Routing And Soft Load Balance Service
Design & Implement

**构想（Concept）
在ISO OIS模型网络层里的两台主机要想互相通信，首先需要知道对方的IP的地址。
在一个分布式的系统环境中，一个服务可能就会多台主机来提供，我们称之为集群（Cluster）。
一个服务请求转发送到哪一台以及如何发送就成了一个现实问题。
在早期，人们是通过一种叫作网络负载器来实现的（如F5），但F5毕竟是硬件，配置、管理及维护都相当不便，而且价格不菲。
后来淘宝开发了LVS（Linux Virtual Server），虽然它可以代替F5，但仍然是充当一个网关（Gateway）的角色，
因此所有的请求都会经过它，随着流量的增加，它就会成为一个瓶颈，何况LVS开发的主要目前不是为内部集群服务的，而是外部接入网关。

   A  |  B
      |
      |
在LB/LVS模型下通信的双方机器是互不可见的，仅通过暴露的VIP进行通信



** 原理（How it works）
VIPServer构想之初就被定义为“一种集群路由软件负载均衡系统”，所以我们想做的就是一种使用普通PC机的为各个集群间提供路由负载服务的系统。
在种构系统下，目的机的地址首先被注册到VIPServer上，之后主机先向VIPServer查询路由信息，然后便根据此路由信息向目的地址发起直连，
完全不需要第三方的参与，不存在通信瓶颈，达到网络利用的最大化。
而这VIPServer这边对结点的状态进行维护，客户端也要定期更新Server端信息，确保路正确，如下图所示：


VIPServer原理图

** 设计、挑战与实现（Design, Challenge and Implement）
因此根据我们的设想，它必须满足：
1.它必须能对路由信息负责，所有结点的配置变更都需要自动且及实的响应
（比如某台机器挂了，那么我们就不能路由到那里去了；某个集群机器上线了，我们就要在自己的路由表里增加此结点）
2.它必须能让应用友好接入，实现最少代码修改成本，且支持自定义配置；

3.它必须方便部署，无特殊配置需要；

4.它必须方便管理、维护，有统一配置入口；

5.它必须有非常好的容灾特性，不能因我们故障导致连接不通或请求阻塞。

第3点：比较好实现，我们将VIPServer分为了Server与Client两端，Server端使用普通web应用，其在淘宝非常成熟，而且有很多自动化工具，部署起来需要的仅仅是一个war包而已。

挑战1：如何自动响应配置变更，减少人力干涉
第1、4点：我们设计了一个web 操作界面，实现对配置信息的集中管理，只需要点几下，就可以轻松对结点信息进行增删改查。但有些PE仍然觉得很麻烦，因为他们可能会管理几十个集群，每个集群又有几十台机器。为了解决这个问题，这样成千上百的IP用手工来进行输入的确是非常麻烦。为此我们引入了armory里nodegroup的概念，在armory系统中，每个app都有自己的group名称，只要提供group名称，就可以查询到机器列表信息，这样一来，PE在注册自己的应用的时候，只需要提供一个nodegroup，我们就可会定期地去更新该app的主机列表，机器列表抓取、机器上线不再需要PE的介入了。

挑战2：如何准确快速响应机器结点故障，最大力度减少路由失败
那么机器下线、故障又怎么办呢？这点看似与前面的问题相同，其实不然，对于机器上线，我们要实时性的要求不是那么高，10分钟、30分钟甚至几小生效都没有问题。
但机器故障、下线就不行了。为了解决这个问题我们设计Server对每个机器结点进行健康检测，我们为每个接入的app设计了一个健康检测任务，每个健康检测任务由一个单程scheduler完全异步执行HTTP或TCP健康检测，间隔3秒。
但这样并不完美，实际我们发现有时候网络不稳定，很可能造因为网络延迟而造成判断不准确。为了解决这个问题当第一次失败的时候，我们会启动一个fast check任务，间隔500ms，如果两次fast check仍然失败，我们才认为其正的失败。
当从失败状态转变成成功状态时也是三次检测才成功，但没有fast check。我们这样做的想法是快速下线，但慢慢上线，因此上线慢不问题不太，至少不会路由失败，但如果测试成功也采用fast check，就会造成过多的fast check任务，
如果网络继续不稳定的话，就可能导致fast check堆积，大大影响Server端的性能。根据实际压测结果，我们的服务器单机能支持2w个IP的健康检查，性能非常高。

健康检测有限状态机

挑战3：如何保证客户端高性能、高可靠性
第2、5点：这其实是一个很矛盾的问题，因为我们又不想像F5那样充当网络网关，干涉机器间的网络传输，又希望对用户的网络请求进行路由控制。最开始我们想的是编写一个VIPServer的客户端，但这个客户必须有很高的性能，不能因为我们慢而已阻塞其请求，因此我们精心设计了路由信息更新，每当一个线程发现数据过期时，我们只会放行该线程去执行更新任务，其它线程仍然使用老数据，这个的难点在于，对某个应用的路由信息访问会存在非常大的大并发量。在Java上我们可以使用concurrent hash map对解决，但C/C++里我们又怎么办呢？这里我们参考了IBM的一篇论文（http://www.almaden.ibm.com/laborday/people/m/michael/spaa-2002.pdf），完全自己实现了C语言上的lock-free hash table（参见：http://www.atatech.org/article/detail/7966/0），经实际测试，根据C客户端的压测报告，TPS高达1700W。同时，为了客户端更新请求泛洪，我们在服务器返回数据时指向客户端需要缓存的时间，也就是说客户端在自己缓存过期前，不会向服务端请求更新数据，这样就大大减少了server的负担。但这样随着客户端的增加，服务器的负担仍然会越来越大。后期，我们为了进一步减少服务器的负担，使用了HTTP长轮询的方式，在这种方式下，每个客户端与服务器只会维持一个TCP长连接。正常性能下，服务器不会立即返回，客户端就会一直挂着，直到某个应用的路由信息发生变化或超时，服务才会返回。为了保证变更不丢失，客户端在请求的时候还需要附带上上次数据版本号。

t_14279_1381985901_1601372883.png

除了性能之外，我们的客户最关心的就是稳定性问题。我们首先列举了VIPServer可能出现的故障点：

1.      VIPServer集群中一台服务器故障；

2.      VIPServer集群所有服务器均故障；

3.      VIPServer配置出错，配置清空；

4.      Client故障，无法服务。

第1、2点：VIPServer 集群不可用的问题，如上所述我们为客户端增加了本地缓存。在HTTP长轮询模式中，我们会单独起一个线程与做，它与主服务线程毫无关系，所以这个更新过程更是完全异步的，这样就完全不会阻塞用户进程了。

第3、4点：为了对客户端本地的缓存进行保护，每次客户端缓存更新时，我们还会将其写入磁盘缓存。这样万一如果Client故障而且Server也连不通的情况下，我们也可以通过重启来进行快速恢复服务，保证网络调用正常。我们精心设计Client端是不接受空数据的，也就是说如果某个时候即便Server配置被删除或者配置错误，Client端的数据也不会被覆盖，从而保证最基本的调用通畅。对于DNS-F客户端如果万一Client挂掉，我们也可以通过快速重启来解决。

t_14279_1381985930_1496454697.png

VIPServer数据更新流程

挑战4：面对众多语言、架构，如何做到统一接入
VIPServer服务的对象是整个集团，因此会遇到各种各样的客户端，如PHP、Java、C/C++、JavaScript等等。因为考虑到客户端其实还是挺简单的，刚开始我们还可以针对某种语言编写对应的客户端，但随着功能的添加，各种客户端的维护就成为了非常头疼的问题。另一方面，在实现推广中我们发现，有些应用如TANX是使用多进程来进行网络访问的，原有的内存模型显然无法实现多进程间的缓存共享，如果为一个进程单独开一个client的话，这样显然又是相当浪费的。为了解决这个问题，我们提出了DNS Filter的概念，也就是我们会在客户端本地单独启一个进程，这个进程占用53号端口。客户在使用的时候，首先更改resolv.conf，将首先DNS指向127.0.0.1，即本机。这样我们的进程就能过滤DNS解析请求了，实现对上层客户端的透明支撑,如果不是我们管辖的域名，我们就直接转发出去。通过TANX应用的实际使用，这种方案切实可行。根据测试提供的压测报告，本客户端在当前单线程的时候TPS能支持至1.8w，我们认为这对大多数情况都是够用的了。

而在于服务端，我们使用了最通用的HTTP+JSON做为信息传输的载体，完全不依赖客户端上层应用。同时我们基于此还开放了SDK，同样基于HTTP，对于可以非常对自己的域名进行自动化维护。

VIPServer V.S. ConfigServer
首先从服务内容上：ConfigServer充当的一个配置中心的角色，它与Diamond一样，只是承担的是状态敏感的配置信息，其核心在于状态敏感配置的同步。而VIPServer的服务内容是负载，其核心是保证有效路由与路由控制。比如说使用VIPServer我可以让某个应用不能调用，或者降低调用的比重。所以在我看来，他们两个并没有太多的重复，用户使用VIPServer不一定使用ConfigServer，使用ConfigServer也不一定使用VIPServer。

其次从数据角度上：ConfigServer使用的是TCP主动推送方式，数据是非持久的；VIPServer采用的则是TCP+UDP推拉结合的方式，数据持久在Diamond上。在数据有效性验证上，ConfigServer采用与连接绑定的方式，一旦连接断开就意味着数据失效且从内存移除，而VIPServer采用的则是主动检测的方式，目前支持HTTP与TCP两种方式，如果检测失败，则认为数据失效但不从磁盘上删除。

最后从将来的发展来看：ConfigServer会继续着重在配置信息推送上，重点服务HSF；而VIPServer则会向路由访问控制方向发展，支持等自定义路由控制，要做到只需在VIPServer进行一些简单配置，就可以实现对应用相互调用的控制。

现状（Today’s VIPServer）
VIPServer刚诞生不久，线上已接入10个应用，路由结点50台+，客户机器100台+，其中包括TANX这种日调用量超40亿的大客户）都已开始使用VIPServer来实现集群间直连调用。更为关键的是很多大型应用，如阿里旺旺、阿里云计算都对VIPServer产生了强烈的兴趣，特别是阿里云，其中有一个应用ESC， 1w+的机器正准备接入。

未来（Future’s VIPServer）


我们将接管现有的LVS方案，实现集群间完全透明的端到端访问，大大减少运营成本，还将提供访问控制与流量，让用户轻松控制客户端的调用访问，实现功能全面的集群路由及软件负载均衡服务。