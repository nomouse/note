* redis

** redis问题
*** redis特点
1. 内存+持久化; 2. 多种数据结构; 3.单线程IO非阻塞多路复用

*** redis常用数据结构和场景
**** kv
**** hash(kkv)
**** list
**** set
**** zset


** redis高级问题
听起来都颇有道理是不是？对于 Redis 集群而言，如果业务机器就永远保持在三五十个以内；连接数始终维持在几千以内；单个数据结构（List、Hash、Set 等）的字段维持在千以内；总体请求维持在单节点三五万以内；而且请求基本上都是简单的 O(1) 或者 O(lgN) 的查询。那这个 Redis 服务确实会很稳定的，可能业务一辈子都不需要更快更好的 Redis 服务了

● 某用户打满了 Redis 的服务，主线程 CPU 使用率 100% 后业务开始卡顿，为了解决问题，用户紧急在控制台进行扩容。然而 Redis 主线程过于繁忙，新的主备关系很难建立，建立后数据同步进度特别慢。用户特别着急，但我们除了让用户降低流量外束手无策（限流对业务伤害太大，当下只是卡顿，限流会大面积不可用）。最后怎么解决的？说来惭愧，等用户流量自然降低后才升配成功的。
● 某用户的访问存在间歇的峰值，主线程负载时不时飙高，导致管控采集服务监控的请求也时不时超时。最终造成了监控数据的曲线出现断点，引发了用户投诉。为什么超时？因为监控链路和数据链路都是主线程处理的。当前只是在社区版上把探活端口和逻辑移动到了另外的线程，监控和管控仍在主线程。（正在改造，但是还是遇到了很多困难。因为 Redis 单线程，所以数据结构都是线程不安全的。所以在别的线程获取信息要么加锁，要么主线程提前生成好。提前生成看起来挺好是吧？但是主线程执行一个慢查询的时候，也就不会执行生成的定时任务去生成这个信息了，拿到的也是旧的）。
● 读写分离场景下，Redis 链式挂载了多个同步只读节点，从 Master 开始直到最后的只读节点，数据更新同步的一致性逐渐降低（链式逐个同步存在时差）。当 Master 遇到大流量打满 CPU 时，容易造成主从断开（堆积的同步请求太多或者心跳失败），然后触发链式同步断开后逐个节点全量同步。然而 CPU 是打满的，同步断开后更难建立了，最终难以恢复。为什么是链式同步不是星型同步？链式直挂载一个只读节点都会出问题还敢星型啊？我们能做什么？检测到这个情况后 Proxy 隔离数据不同步的从节点。但是！这是读写分离实例啊，流量大到了主从都断开了，隔离了更多的只读节点这是要让系统雪崩吗？
● Redis 作为缓存使用时，为了缓存一致性，数据都是有过期时间兜底的。如果一个节点长期以高 CPU 运行时，过期检查和清理的定时任务就得不到足够多的优先级去执行，累积的过期数据过多后达到内存上限就会淘汰。业务会发现明明存在很多过期数据，但是淘汰的却不一定是过期的数据。
● Redis 的内存统计是进程级别的，没有分区域（数据区，元数据区、Client buf 等等），当遇到了一些网络延迟大或者客户端/服务端处理缓慢的情况时，会造成较多的 Client buf 占用。当内存统计达到设置的阈值后就开始淘汰用户数据了。那为什么不把统计分开？统计各个单独区域有这么难吗？技术上不难，但是这是个繁琐的工作，但凡漏掉一处就会引发大问题。Redis 怎么统计全局的？Redis 内存分配和释放在 malloc 和 free 上包了一层，有自己的 zmalloc/zfree，在申请和释放的时候做的。那 Redis 后台还是有几个 BIO 线程啊，怎么保证统计准确？难道用原子变量？对，就是原子变量，每次内存申请和释放都是原子加减一次。如果再区分每个区域，都包装一个 xxx_malloc/xxx_free，先不说繁琐性，原子操作的翻倍会带来性能的进一步降低（原子统计有些按照 Cache Line 对齐的小技巧减少 False Sharing，但是高频原子操作带来的性能损失还是尽量要避免的）。
● 某直播用户，采用 Pub/Sub 机制进行弹幕的广播，需要 1 对 N 的广播能力。但是 Redis 的单线程负担不起（1 对多等于请求放大），最终业务采用了一主几十从的架构去做链式的广播消息推送，这个架构特别复杂且运维难度很高。如果 Redis 能支持1对几十甚至几百的广播能力就不用这么多从节点了。


